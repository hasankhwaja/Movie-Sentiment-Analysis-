{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5eb1d22-f0db-4cb4-b53d-09d372584268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "\n",
    "data = pd.read_csv('train.tsv', sep = '\\t') \n",
    "data_train = pd.read_csv('test.tsv', sep = '\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaaf8b14-b51f-4142-b893-9d0ef57bdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "\n",
    "def remove_tags(string): \n",
    "    removelist = \"\"\n",
    "    result = re.sub('<.*?>', '', string)  # remove HTML tags\n",
    "    result = re.sub('https://.*', '', result)   # Remove URLs\n",
    "    result = re.sub(r'[^\\w\\s]', ' ', result)    # Remove non-alphanumeric characters\n",
    "    result = result.lower()\n",
    "    return result \n",
    "\n",
    "# remove stop words that hold no meaning for sentiment \n",
    "data['Phrase']=data['Phrase'].apply(lambda cw : remove_tags(cw))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['Phrase'] = data['Phrase'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fcb248e-a38d-4c69-a6eb-ed98cbfb64ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>series escapade demonstrating adage good goose...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>series escapade demonstrating adage good goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  series escapade demonstrating adage good goose...   \n",
       "1         2           1    series escapade demonstrating adage good goose    \n",
       "2         3           1                                            series    \n",
       "3         4           1                                                      \n",
       "4         5           1                                            series    \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform lemmatization to find root form of words \n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    st = \"\"\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "    return st\n",
    "data['Phrase'] = data.Phrase.apply(lemmatize_text) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dfd8be6-9b02-40e2-9655-1ec061d99f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "695f5a46-fe73-448f-8c19-e56a972e5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "phrases = data['Phrase'].values\n",
    "sentiment_score = data['Sentiment'].values\n",
    "train_phrases, test_phrases, train_labels, test_labels = train_test_split(phrases, sentiment_score) \n",
    "\n",
    "# start vectorization \n",
    "vector = CountVectorizer(max_features = 3000) \n",
    "X = vector.fit_transform(train_phrases) \n",
    "vocab = vector.get_feature_names_out() \n",
    "\n",
    "# initialize word counts \n",
    "X = X.toarray() \n",
    "word_counts = {}\n",
    "for i in range(5):\n",
    "    word_counts[i] = defaultdict(lambda: 0)\n",
    "\n",
    "# store frequency of word counts in a dictionary \n",
    "for j in range(X.shape[0]):\n",
    "    i = train_labels[j]\n",
    "    for h in range(len(vocab)): \n",
    "        word_counts[i][vocab[h]] += X[j][h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e828525e-035b-4cc0-a797-3dda997de5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform laplace smoothing the difference of words between training and test set\n",
    "\n",
    "def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label): \n",
    "    a = word_counts[text_label][word] + 1 \n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b) \n",
    "\n",
    "# define fit and predict functions for the classifier \n",
    "\n",
    "def group_by_label(x, y, labels): \n",
    "    dict_data = defaultdict(list) \n",
    "    for l in labels: \n",
    "        dict_data[l] = x[np.where(y == l)]\n",
    "    return dict_data\n",
    "\n",
    "def fit(x, y, labels): \n",
    "    n_label_items = {} \n",
    "    log_label_priors = {} \n",
    "    n = len(x) \n",
    "    grouped_data = group_by_label(x, y, labels) \n",
    "    for l, dict_data in grouped_data.items(): \n",
    "        n_label_items[l] = len(dict_data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n) \n",
    "    return n_label_items, log_label_priors\n",
    "\n",
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "        words = set(w_tokenizer.tokenize(text))\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        result.append(max(label_scores, key = label_scores.get))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62d57405-6ca2-440c-a963-05683d3a1c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction on test set:  0.543842112008202\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training data\n",
    "n_label_items, log_label_priors = fit(train_phrases, train_labels, labels)\n",
    "\n",
    "# Initialize and compute word counts for the training set\n",
    "vector = CountVectorizer(max_features=3000) \n",
    "X_train = vector.fit_transform(train_phrases)\n",
    "vocab = vector.get_feature_names_out()  # Update vocab for training set\n",
    "\n",
    "word_counts = {}\n",
    "for i in labels:  # Labels are [0, 1, 2, 3, 4]\n",
    "    word_counts[i] = defaultdict(lambda: 0) # account for unseen words  \n",
    "    \n",
    "# Populate word counts for each sentiment class\n",
    "X_train = X_train.toarray()\n",
    "for j in range(X_train.shape[0]):\n",
    "    sentiment_class = train_labels[j]\n",
    "    for h in range(len(vocab)): \n",
    "        word_counts[sentiment_class][vocab[h]] += X_train[j][h]\n",
    "\n",
    "# Predict on the test set\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_phrases)\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy of prediction on test set: \", accuracy_score(test_labels, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2afa9-5aec-4e0e-be38-a349330fd3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
